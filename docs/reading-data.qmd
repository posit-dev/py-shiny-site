---
title: Reading data in Shiny
editor:
  markdown:
    wrap: sentence
lightbox:
  effect: fade
---

From CSVs to parquet files to cloud-hosted databases, if you can access it from Python, you can access it from Shiny. We won't go through every possible configuration here, but we'll choose a handful to explore. Our examples will start simple and build in complexity.

## Reading data into memory {#read-into-memory}

A common pattern in Shiny apps is reading in data from a file on your computer, such as a CSV. We use an example CSV in the example below, but the strategy holds for other file formats you might want to use.

### Examples
::: {.panel-tabset .panel-pills}

### Polars
```python
df = pl.read_csv(Path(__file__).parent / "test_data.csv")
```

### Pandas
```python
df = pd.read_csv(Path(__file__).parent / "test_data.csv")
```

### DuckDB
```python
df = duckdb.read_csv(Path(__file__).parent / "test_data.csv").df()
```
:::

::: callout-note
### Remember: Put your data in a module!

For better performance in situations where you are loading an entire dataset into memory, it is best to load your data from within a separate module and import it in your `app.py` file. This makes sure it is only loaded once. For more info why, see the Express documentation [here](express-in-depth.qmd#shared-objects) but for now, let's just make sure we read our csv in a separate module, then import it in our `app.py`.

<details>
<summary>Show example code</summary>

```{.python filename="app.py"}
from shiny.express import ui, render
import data_load_module

ui.page_opts(title="Data", full_width=True, id="page")

@render.data_frame
def results_df():
    return render.DataGrid(data_load_module.df)
```
```{.python filename="load_data.py"}
import polars as pl
from pathlib import Path

df = pl.read_csv(Path(__file__).parent / "test_data.csv")
```
</details>
:::

## Databases and larger-than-memory data {#read-db}
Some datasets are too large to fit cleanly in memory or don't live on our computer. Perhaps you want to connect to a remote database or perform complicated filtering operations on a large dataset.

### Examples
::: {.panel-tabset .panel-pills}
### Polars LazyFrame
```python
df = pl.scan_csv(Path(__file__).parent / "test_data.csv")

# Example query
query_result = df.head(100).collect()
```
### Ibis
Ibis provides a data table interface for a wide variety of backends, including in-memory data via sources like DuckDB or SQLite and remote data, like Postgres.
We'll use Postgres in the below example, but know that Ibis supports many backends.
```bash
pip install 'ibis-framework[postgres]'
```

```python
import ibis

con = ibis.postgres.connect(
    user="username",
    password="password",
    host="hostname",
    port=5432,
    database="database",
)
my_table = con.table("mytablename")

# This is when Ibis will perform your query operations
query_result = my_table.head(100)

```

### SQLAlchemy
SQLAlchemy allows for more fine-grained SQL usage with either plain SQL text execution or SQLAlchemy's own Object-Relational Mapping syntax, which allows you to use a similar syntax to Polars and Ibis.

SQLAlchemy is the most flexible method of all of these, but with this flexibility comes much greater complexity.
```python
from sqlalchemy import create_engine, text

# Note: You will need to also install psycopg2 (or specify another supported connector)
engine = create_engine('postgresql://user:password@hostname/database_name')
with engine.connect() as conn:
    result = conn.execute (text("SELECT * FROM tablename LIMIT 100"))
```
:::

In both cases, we define our query (our data manipulations) and then rely on an external 'engine' to optimize it, run it, and provide us with the results.

We'll go over two examples here:

1. You have a large amount of data available on your computer that you want to use in a Shiny app. This is a great case for Polars *Lazy API*, which optimizes data manipulation tasks to run as quickly and using as little memory as possible.
2. You are connecting to a remote database. Your query will run remotely on that database and return a result to you (ex. Ibis with remote datastore, SQLAlchemy)

Let's start with the first case.

### Large datasets and Polars' Lazy API

#### Lazy Evaluation

Polars has a great toolset for efficiently working with large data called the *lazy API*. The lazy API takes your query as a whole instead of line-by-line and processes only what's needed when it's needed. It does this using its internal query optimizer and uses streaming to work with data sets larger than could otherwise fit in memory.

Let's modify our work from above to use the Lazy API.

```{.python filename="load_data.py"}
import polars as pl
from pathlib import Path

# Use `scan_*` instead of `read_*` to use the lazy API
df = pl.scan_parquet(Path(__file__).parent / "filename.parquet")

```
```{.python filename="app.py"}
from shiny.express import ui, render
import load_data_module

ui.page_opts(title="My Table", full_width=True, id="page")


@render.data_frame
def results_df():
    # Use `.collect()` to get our result (in this case, getting the first 100 rows)
    return render.DataGrid(load_data_module.df.head(100).collect())
```

You might notice that the syntax has changed a little bit. We use `scan_parquet()` instead of `read_parquet` to return a Lazy Frame instead of a DataFrame. This means that when we import our data in the module, we're not actually loading it in yet.
Polars waits until it sees the `.collect()` to execute the sequence of commands to
1. Read in the data
2. Get the first 100 rows (`.head(100)`) and then
3. `.collect()` the results.

In this case, that's immediately since we're rendering the data frame as soon as we load the app.
However, there may be cases where you delay execution. For example, you might wait until after you've collected user-inputs or users have clicked a button.

Let's look at an example with the [The Weather Dataset](https://www.kaggle.com/datasets/guillemservera/global-daily-climate-data).
This dataset is much, much bigger (nearly 28M rows) and so we'll take full advantage of Polars Lazy API to keep our app fast.

::: callout-note
### Modules and Laziness
Remember how before we had to put our data in a module? Now that we're only loading our data and querying when we need it (when we've called `.collect`), we don't need to worry about that!

If you're using the Lazy API or reading from a database, you can create that connection to your data in a module, or not. It's up to you.
:::

#### Example Application

Here's a simple app that takes advantage of Polars Lazy API:

TODO: Add run it in shiny live link?

```python
from shiny.express import ui, render, input
import polars as pl

# Scan the parquet file. It won't be truly read until it's needed.
df = pl.scan_parquet("./daily_weather.parquet")

ui.page_opts(title="Weather", full_width=True, id="page")

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Fall", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city",
    "City",
    choices=df.select("city_name").unique().collect().to_series().to_list(),
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    results = (
        df.filter(pl.col("city_name") == input.city())
        .filter(pl.col("season").is_in(input.season()))
        .collect()
    )
    return render.DataGrid(results)

```

We can make this same applciation using Ibis and a remote data store:





END IMPORTING DATA ARTICLE

Top level:
Importing Data
    Potentially have a 'my data lives remotely' skip down to bottom link
    x * In-memory data importing
    x * Out of memory/lazyframe/remote data
        * Callout a few different options
            * Ibis
            * SQLAlchemy
            * Polars Lazyframe
            * Something else?
        * Lazyframe
            * How we interact with this in the app based on when it becomes in memory
        * Now our data is totally remote:
            * Ibis example (local connect) supports in memory too callout
            * SQLAlchemy remote (postgres? in AWS RDS)
            * Pins callout supports in memory too callout
            * Recommendation for connecting to data warehouses

Doing stuff to data - callout to mutability article
Persistant Storage/Writing
* Callout connect cloud and support for secrets and persistent storage, link to docs, tehre's a talk that aleks chisolm did at conf




### Reactivity and display


### Advanced: Streaming & Batching, Caching



## Read from a remote DB or source

What if your data is already in a database or hosted somewhere else on the cloud? Shiny can interact with that too.

We've collected a few examples below, but any data source with the appropriate Python drivers should be able to work with Shiny.




::: {.panel-tabset .panel-pills}

#### Read from an RDS Postgres Store
```python
# Examples
```

#### Connect to Snowflake

#### Connect to Databricks

#### Read from a shared object store using Pins
:::


::: callout-note
### Remember: Be careful of SQL injection vulnerabilities

When you start giving users more access to your data, you have to be careful of SQL injection attacks.


:::


## What about writing to a data store? When to consider persistent storage




END SO FAR




For instructions on XYZ, see HEADERLINK

We've outlined two main cases below, one SQL case and one NoSQL case.

## Callout on SQL Injection Attacks?

## Example App

Let's start from the simplest possible case: an app with no persistent storage.
Now, what if we want to save these results somewhere?

```python
from shiny import App, Inputs, Outputs, Session, ui, reactive, render
import pandas as pd

app_ui = ui.page_fluid(
    ui.h2("Submitted Results"),
    ui.output_data_frame("results_df"),
    ui.input_text("text", "Enter text", value=""),
    ui.input_checkbox("checkbox", "I like checkboxes"),
    ui.input_slider("slider", "My favorite number is:", min=0, max=100, value=50),
    ui.input_action_button("submit", "Submit"),
)


def server(input: Inputs, output: Outputs, session: Session):
    df = reactive.value(pd.DataFrame(columns=["text", "checkbox", "slider"]))

    @reactive.effect
    @reactive.event(input.submit)
    def results():
        row = {
            "text": input.text(),
            "checkbox": input.checkbox(),
            "slider": input.slider(),
        }
        row_df = pd.DataFrame([row])
        new_df = pd.concat([df.get(), row_df], ignore_index=True)
        df.set(new_df)

    @render.data_frame
    def results_df():
        return render.DataGrid(df.get())


app = App(app_ui, server)
```

## Connecting a database with DuckDB or Polars

### Setup

### Example App
::: {.panel-tabset .panel-pills}

#### DuckDB
```python
from shiny import App, Inputs, Outputs, Session, ui, reactive, render
import duckdb

con = duckdb.connect("test_db.db")

app_ui = ui.page_fluid(
    ui.h2("Submitted Results"),
    ui.output_data_frame("results_df"),
    ui.input_text("text", "Enter text", value=""),
    ui.input_checkbox("checkbox", "I like checkboxes"),
    ui.input_slider("slider", "My favorite number is:", min=0, max=100, value=50),
    ui.input_action_button("submit", "Submit"),
)


def server(input: Inputs, output: Outputs, session: Session):
    # Initialize a reactive value so we start with the table visible.
    df = reactive.value(con.sql("SELECT * FROM data").df())

    @reactive.effect
    @reactive.event(input.submit)
    def save():
        row = [input.text(), input.checkbox(), input.slider()]

        # We use a prepared statement here to avoid SQL injection
        con.execute("INSERT INTO data VALUES (?, ?, ?)", row)
        con.commit()

        # Set the reactive value so the table updates
        df.set(con.sql("SELECT * FROM data").df())

    @render.data_frame
    def results_df():
        return df()


app = App(app_ui, server, debug=True)
```





### Example Applications
::: {.panel-tabset .panel-pills}

#### Polars


#### DuckDB
Outliers app: Reads and writes from DuckDB https://github.com/skaltman/outliers-app-db-python/blob/main/app.py
Database Explorer: https://shiny.posit.co/py/templates/database-explorer/

#### Pandas
AWS-Community-Builders-App: Process data from a CSV and displays it https://github.com/robertgv/aws-community-builders-dashboard

#### Sqlite3
Database monitoring app: https://github.com/posit-dev/py-shiny-templates/tree/main/monitor-database
