---
title: Reading data in Shiny
editor:
  markdown:
    wrap: sentence
lightbox:
  effect: fade
---

From CSVs to parquet files to cloud-hosted databases, if you can access it from Python, you can access it from Shiny. We won't go through every possible configuration here, but we'll choose a handful to explore. Our examples will start simple and build in complexity.

## Reading data into memory {#read-into-memory}

A common pattern in Shiny apps is reading in data from a file on your computer, such as a CSV. We use an example CSV in the example below, but the strategy holds for other file formats you might want to use.

### Examples
::: {.panel-tabset .panel-pills}

### Polars
[Polars](https://pola.rs/) is an open-source library for data manipulation. It provides tooling to make data processing fast and intuitive. It is particularly helpful for large datasets, as its optimization engine can significantly speed up costly processes.

```python
df = pl.read_csv(Path(__file__).parent / "test_data.csv")
```

### Pandas
[Pandas](https://pandas.pydata.org/) is one of the most widely used open-source data analysis libraries.

```python
df = pd.read_csv(Path(__file__).parent / "test_data.csv")
```

### DuckDB
[DuckDB](https://duckdb.org/) is an open-source, in process database system which allows you to perform data analysis tasks quickly and with SQL syntax. Its memory usage optimizations make complex calculations quicker to run.

```python
df = duckdb.read_csv(Path(__file__).parent / "test_data.csv").df()
```
:::

::: callout-note
### Remember: Put your data in a module!

For better performance in situations where you are loading an entire dataset into memory, it is best to load your data from within a separate module and import it in your `app.py` file. This makes sure it is only loaded once. For more info why, see the Express documentation [here](express-in-depth.qmd#shared-objects) but for now, let's just make sure we read our csv in a separate module, then import it in our `app.py`.

<details>
<summary>Show example code</summary>

```{.python filename="load_data.py"}
import polars as pl
from pathlib import Path

df = pl.read_csv(Path(__file__).parent / "test_data.csv")
```

```{.python filename="app.py"}
from shiny.express import ui, render
import data_load_module

ui.page_opts(title="Data", full_width=True, id="page")

@render.data_frame
def results_df():
    return render.DataGrid(data_load_module.df)
```
</details>
:::

## Databases and larger-than-memory data {#read-db}
Data that fits in memory makes it simpler to filter and perform calculations based on reactive inputs. If you can have your data in memory or on disk, we recommend you start there and make use of Polars' Lazy API when possible to use memory effectively.

Some datasets are too large to fit cleanly in memory or don't live on our computer. Perhaps you want to connect to a remote database or perform complicated filtering operations on a large dataset. In these cases, knowing when you are loading data into memory and where and when you are performing calculations becomes more critial.

There are two strategies you can use to make working with large data fast and efficient:

1. Use tools like Polars' Lazy API to perform calculations only when needed and with internal optimization tooling. Load your data from storage on-disk using formats optimized for large data, like Parquet.

2. Store your data in a remote database and bring only what you need into memory using a tool like Ibis, SQLAlchemy, or another database connection library.

We'll go over a few examples below.

### Examples
::: {.panel-tabset .panel-pills}
### Polars LazyFrame
```python
df = pl.scan_csv(Path(__file__).parent / "test_data.csv")

# Example query
query_result = df.head(100).collect()
```

### Ibis
[Ibis](https://ibis-project.org/) provides a shared dataframe interface for a wide variety of backends, including DuckDB, Postgres, BigQuery, etc. Ibis uses a form of lazy evaluation by default, but it tends to be slower than Polars.

We'll use Postgres in the below example to demonstrate connecting to a remote data source.

```bash
pip install 'ibis-framework[postgres]'
```

```python
import ibis

con = ibis.postgres.connect(
    user="username",
    password="password",
    host="hostname",
    port=5432,
    database="database",
)
my_table = con.table("mytablename")

# This is when Ibis will perform your query operations
query_result = my_table.head(100)

```

### SQLAlchemy
SQLAlchemy allows for more fine-grained SQL usage with either plain SQL text execution or SQLAlchemy's own Object-Relational Mapping syntax, which allows you to use a similar syntax to Polars and Ibis. We use raw SQL in the below example to reduce the complexity of the example.

SQLAlchemy is the most flexible method of all of these, but with this flexibility comes much greater complexity.
```python
from sqlalchemy import create_engine, text

# Note: You will need to also install psycopg2 (or specify another supported connector)
engine = create_engine('postgresql://user:password@hostname/database_name')
with engine.connect() as conn:
    result = conn.execute(text("SELECT * FROM tablename LIMIT 100"))
```
:::

In both cases, we define our query (our data manipulations) and then rely on an external 'engine' to optimize it, run it, and provide us with the results, at which point those results are loaded into memory.

We'll go over two examples below.

* The first application loads data from a file and uses *Polars' LazyFrames* for highly optimized data manipulation.

* The second connects to a Postgres database using *Ibis* to pass user-inputs to the database, perform calculations, and then return the result back to the application.

Let's start with the first case.

### Large datasets and Polars' Lazy API

#### Lazy Evaluation

Polars has a great toolset for efficiently working with large data called the *lazy API*. The lazy API takes your query as a whole instead of line-by-line and processes only what's needed when it's needed. It does this using its internal query optimizer and uses streaming to work with data sets larger than could otherwise fit in memory.

Let's modify our work from above to use the Lazy API.

```{.python filename="app.py"}
from shiny.express import ui, render
import polars as pl
from pathlib import Path

# Use `scan_*` instead of `read_*` to use the lazy API
# Notice we're not in a module anymore. More on that below.
df = pl.scan_parquet(Path(__file__).parent / "filename.parquet")

ui.page_opts(title="My Table", full_width=True, id="page")

@render.data_frame
def results_df():
    # Use `.collect()` to get our result (in this case, getting the first 100 rows)
    return render.DataGrid(load_data_module.df.head(100).collect())
```

You might notice that the syntax has changed a little bit. We use `scan_parquet()` instead of `read_parquet` to return a Lazy Frame instead of a DataFrame.

Polars waits until it sees the `.collect()` to execute the sequence of commands to

1. Use the reference to the data that was created by `scan_parquet()`

2. Get the first 100 rows (`.head(100)`) and then

3. `.collect()` the results into memory.

This is also what allows us to move connecting to our data source back into the main application instead of in a module, because when we use `scan_*()`, we are getting a pointer to the data, not loading it into memory.

In this example, we collect our results as soon as we load the application so we can render the data frame immediately. However, there may be cases where you delay execution. For example, you might wait until after you've collected user-inputs or users have clicked a button.

Let's look at an example with the [The Weather Dataset](https://www.kaggle.com/datasets/guillemservera/global-daily-climate-data).
This dataset is much, much bigger (nearly 28M rows) and so we'll take full advantage of Polars Lazy API to keep our app fast.

::: callout-note
### Modules and Laziness
Remember how before we had to put our data in a module? Now that we're only loading our data and querying when we need it (when we've called `.collect`), we don't need to worry about that!

If you're using the Lazy API or reading from a database, you can create that connection to your data in a module, or not. It's up to you.
:::

#### Example Application: Polars

Here's a simple app that takes advantage of Polars Lazy API. In this example, we have the dataset on our computer, but we could also easily load from a shared, remote location using a tool like [Pins](https://rstudio.github.io/pins-python/) or a package like [boto3](https://docs.aws.amazon.com/code-library/latest/ug/python_3_s3_code_examples.html) to read from AWS S3. We'll talk more about tools like Pins in our next article when we look at how to *write* to remote data stores.

```python
from shiny.express import ui, render, input
import polars as pl

# Scan the parquet file. It won't be truly read until it's needed.
df = pl.scan_parquet("./daily_weather.parquet")

ui.page_opts(title="Weather", full_width=True, id="page")

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Fall", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city",
    "City",
    choices=df.select("city_name").unique().collect().to_series().to_list(),
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    results = (
        df.filter(pl.col("city_name") == input.city())
        .filter(pl.col("season").is_in(input.season()))
        .collect()
    )
    return render.DataGrid(results)

```

::: callout-note
### You can read from a database in Polars!
Polars can read from a databases too! It's important to note though that Polars essentially reads in the database table like a file, you'll still need to have it on disk (saved locally). For best results, try and scope down your initial query to pull the minimum data needed for your application and if you require many database queries and user inputs, consider a more sophisticated connector.

<details>
<summary>Show example code</summary>

```{.python filename="load_data.py"}
import polars as pl

uri = "postgresql://user:password@hostname/database_name"
# This is where you should scope down your query if you can.
query = "SELECT * FROM weather"

# Notice that this uses `read_*`. Polars cannot read the data lazily from the database, so this will load the entire query into memory.
# That's why we've put this into a module again.
data = pl.read_database_uri(query, uri)
```

```python
from shiny.express import ui, render, input
import polars as pl
from load_data import data

# This creates a LazyFrame from the whole dataset, which we loaded into memory in the module.
df = pl.LazyFrame(data)

ui.page_opts(title="Weather", full_width=True, id="page")

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Fall", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city",
    "City",
    choices=df.select("city_name").unique().collect().to_series().to_list(),
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    results = (
        df.filter(pl.col("city_name") == input.city())
        .filter(pl.col("season").is_in(input.season()))
        .collect()
    )
    return render.DataGrid(results)

```
</details>
:::

### Working with remote data and databases

We can make this same application using a remote datastore. In this example, we'll use Ibis and Postgres, but you could easily substitute another database or a data warehouse like Snowflake or Databricks. Similarly, Ibis is the connector we will demonstrate, but you can substitute whatever database connector works best for your use case.

Regardless of what remote database solution or connector we use here, the anatomy of the application is roughly the same. We will take the user-inputs from the UI and pass them to the database for execution rather than keeping the *whole* dataset in the application for processing.

::: {.callout-tip collapse="true"}
## Advanced: Hybrid in-memory and remote approaches

Working with large data effectively requires a strong awareness of when you are transferring data across the network, how much data is in memory, how computationally intensive your calculations are, and when visualizations and computations are being triggered.

There are scenarios where you may find it compelling to mix and match the discussed approaches.
For example, you may want to download a large dataset on a schedule using Ibis, then load it into your Shiny app from a saved Parquet file.
You might also want to update the data in your app only when new data is available, in which case [reactive polling](https://shiny.posit.co/py/api/core/reactive.poll.html) may be of interset to you.
If you need greater interoperability between libraries like Ibis, Polars, Pandas, and others, [Narwhals](https://narwhals-dev.github.io/narwhals/) may be helpful. It provides a compatibility layer to unify your dataframe syntax.
:::

#### Example Application: Ibis Connecting to Remote Database (Postgres)
```python
from shiny.express import ui, render, input
import ibis

# Connect to the database
con = ibis.postgres.connect(
    user="", password="", host="", port=, database=""
)
df = con.table("weather")

ui.page_opts(title="Weather", full_width=True, id="page")

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Autumn", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city", "City", choices=df[["city_name"]].distinct().execute()["city_name"].tolist()
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    results = df.filter(
        [df.city_name == input.city(), df.season.isin(input.season())]
    ).execute()
    return render.DataGrid(results)
```

This syntax should look pretty familiar! Polars and Ibis have similar syntax, but you'll notice we cue the execution of operations (filters, etc.) on our lazy tables using `.execute()` when we use Ibis.

## Best practices

Now that you have data in your application, it's worthwhile to consider reading up on a few topics.

[Reactive calculations](https://shiny.posit.co/py/api/core/reactive.calc.html#returns) allow us to perform calculations once, when inputs change, and propagate that change into outputs. For example, we could extract our filtering logic from the above application into a reactive calculation so it can be used across the application.

Note, this is using the Polars Lazy API version of the app.
<details>
<summary>Show example code</summary>

```python
from shiny.express import ui, render, input
from shiny import reactive
import polars as pl


df = pl.scan_parquet("./daily_weather.parquet")
ui.page_opts(title="Weather", full_width=True, id="page")

ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Autumn", "Spring"],
    selected="Summer",
)

ui.input_selectize(
    "city",
    "City",
    choices=df.select("city_name").unique().collect().to_series().to_list(),
)

@reactive.calc
def result():
    result_df = df.filter(pl.col("city_name") == input.city())
        .filter(pl.col("season").is_in(input.season()))
        .collect()

    return result_df

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Fall", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city",
    "City",
    choices=df.select("city_name").unique().collect().to_series().to_list(),
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    return render.DataGrid(result())

# Render some other output using `result()` dataframe
@render.data_frame
def results_df_2():
    return render.DataGrid(result().head(1))

# Render some additional outputs using `result()` dataframe
```

</summary>

You might also consider waiting to filter until after the user presses a button.

<details>
<summary>Show example code</summary>

```python
from shiny.express import ui, render, input
from shiny import reactive
import polars as pl


df = pl.scan_parquet("./daily_weather.parquet")
ui.page_opts(title="Weather", full_width=True, id="page")

ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Autumn", "Spring"],
    selected="Summer",
)

ui.input_selectize(
    "city",
    "City",
    choices=df.select("city_name").unique().collect().to_series().to_list(),
)

# Add a submit button
ui.input_action_button("submit", "Submit")

# This calculation won't update until the user hits the submit button
@reactive.calc
@reactive.event(input.submit)
def result():
    result_df = (
        df.filter(pl.col("city_name") == input.city())
        .filter(pl.col("season").is_in(input.season()))
        .collect()
    )

    return result_df

# This will not update until the reactive calc updates after submission
@render.data_frame
def results_df():
    return render.DataGrid(result())

# This will not update until the reactive calc updates after submission
@render.data_frame
def results_df_2():
    return render.DataGrid(result().head(1))
```

</summary>

We're filtering our dataset in a compact way here, but especially if you are storing intermediate steps, it is worth reviewing our article on [mutability](https://shiny.posit.co/py/docs/reactive-mutable.html).

We have numerous full examples and starter templates of dashboards using different datastores. We've sorted them into categories roughly corresponding to the tech stack at the bottom of this article.


## Example Applications {#read-data-examples}
::: {.panel-tabset .panel-pills}

#### DuckDB
Outliers app: https://github.com/skaltman/outliers-app-db-python/blob/main/app.py
Database Explorer: https://shiny.posit.co/py/templates/database-explorer/

#### Pandas
AWS-Community-Builders-App: https://github.com/robertgv/aws-community-builders-dashboard
Restaurant Tips: https://shiny.posit.co/py/templates/dashboard-tips/

#### Sqlite3
Database monitoring app: https://github.com/posit-dev/py-shiny-templates/tree/main/monitor-database
:::

## What about writing to a database or object store?

The scenarios we've covered above are focused on reading in data to an application and performing queries with user-inputs. If you need to *write* data to the database or other remote location, there are additional considerations we'll cover in a separate article on persistant storage in Shiny apps.

