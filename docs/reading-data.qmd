---
title: Reading data in Shiny
editor:
  markdown:
    wrap: sentence
lightbox:
  effect: fade
---

From CSVs to parquet files to cloud-hosted databases, if you can access it from Python, you can access it from Shiny. We won't go through every possible configuration here, but we'll choose a handful to explore. Our examples will start simple and build in complexity.

## Reading data into memory {#read-into-memory}

A common pattern in Shiny apps is reading in data from a file on your computer, such as a CSV. We use an example CSV in the example below, but the strategy holds for other file formats you might want to use.

### Examples
::: {.panel-tabset .panel-pills}

### Polars
[Polars](https://pola.rs/) is an open-source library for data manipulation. It provides tooling to make data processing fast and intuitive. It is particularly helpful for large datasets, as its optimization engine can significantly speed up costly processes.

```python
df = pl.read_csv(Path(__file__).parent / "test_data.csv")
```

### Pandas
[Pandas](https://pandas.pydata.org/) is one of the most widely used open-source data analysis libraries.

```python
df = pd.read_csv(Path(__file__).parent / "test_data.csv")
```

### DuckDB
[DuckDB](https://duckdb.org/) is an open-source, in process database system which allows you to perform data analysis tasks quickly and with SQL syntax. Its memory usage optimizations make complex calculations quicker to run.

```python
df = duckdb.read_csv(Path(__file__).parent / "test_data.csv").df()
```
:::

::: callout-note
### Remember: Put your data in a module!

For better performance in situations where you are loading an entire dataset into memory, it is best to load your data from within a separate module and import it in your `app.py` file. This makes sure it is only loaded once. For more info why, see the Express documentation [here](express-in-depth.qmd#shared-objects) but for now, let's just make sure we read our csv in a separate module, then import it in our `app.py`.

<details>
<summary>Show example code</summary>

```{.python filename="app.py"}
from shiny.express import ui, render
import data_load_module

ui.page_opts(title="Data", full_width=True, id="page")

@render.data_frame
def results_df():
    return render.DataGrid(data_load_module.df)
```
```{.python filename="load_data.py"}
import polars as pl
from pathlib import Path

df = pl.read_csv(Path(__file__).parent / "test_data.csv")
```
</details>
:::

## Databases and larger-than-memory data {#read-db}
Some datasets are too large to fit cleanly in memory or don't live on our computer. Perhaps you want to connect to a remote database or perform complicated filtering operations on a large dataset. In these cases, where and when your database is loaded and operations are performed becomes more important.

There are two strategies you can use to make working with large data fast and efficient:
1. Use tools like Polars' Lazy Frame to perform calculations only when needed and with internal optimization tooling.
2. Store your data in a remote database and bring only what you need into memory using a tool like Ibis, SQLAlchemy, or another database connection library.

We'll go over a few examples below.

### Examples
::: {.panel-tabset .panel-pills}
### Polars LazyFrame
```python
df = pl.scan_csv(Path(__file__).parent / "test_data.csv")

# Example query
query_result = df.head(100).collect()
```

### Ibis
[Ibis](https://ibis-project.org/) provides a shared dataframe interface for a wide variety of backends, including DuckDB, Postgres, BigQuery, etc. Ibis uses a form of lazy evaluation by default, but it tends to be slower than Polars.

We'll use Postgres in the below example to demonstrate connecting to a remote data source.

```bash
pip install 'ibis-framework[postgres]'
```

```python
import ibis

con = ibis.postgres.connect(
    user="username",
    password="password",
    host="hostname",
    port=5432,
    database="database",
)
my_table = con.table("mytablename")

# This is when Ibis will perform your query operations
query_result = my_table.head(100)

```

### SQLAlchemy
SQLAlchemy allows for more fine-grained SQL usage with either plain SQL text execution or SQLAlchemy's own Object-Relational Mapping syntax, which allows you to use a similar syntax to Polars and Ibis.

SQLAlchemy is the most flexible method of all of these, but with this flexibility comes much greater complexity.

#TODO: Add ORM example
```python
from sqlalchemy import create_engine, text

# Note: You will need to also install psycopg2 (or specify another supported connector)
engine = create_engine('postgresql://user:password@hostname/database_name')
with engine.connect() as conn:
    result = conn.execute(text("SELECT * FROM tablename LIMIT 100"))
```
:::

In both cases, we define our query (our data manipulations) and then rely on an external 'engine' to optimize it, run it, and provide us with the results.

We'll go over two examples below:

1. You have a large amount of data available on your computer (or that you can download from an object store) that you want to use in a Shiny app. This is a great case for Polars' *Lazy API*, which optimizes data manipulation tasks to run as quickly and using as little memory as possible.
2. You are connecting to a remote database. Your query will run remotely on that database and return a result to you. Our example will use Ibis for this.

Let's start with the first case.

### Large datasets and Polars' Lazy API

#### Lazy Evaluation

Polars has a great toolset for efficiently working with large data called the *lazy API*. The lazy API takes your query as a whole instead of line-by-line and processes only what's needed when it's needed. It does this using its internal query optimizer and uses streaming to work with data sets larger than could otherwise fit in memory.

Let's modify our work from above to use the Lazy API.

```{.python filename="load_data.py"}
import polars as pl
from pathlib import Path

# Use `scan_*` instead of `read_*` to use the lazy API
df = pl.scan_parquet(Path(__file__).parent / "filename.parquet")
```

```{.python filename="app.py"}
from shiny.express import ui, render
import load_data_module

ui.page_opts(title="My Table", full_width=True, id="page")

@render.data_frame
def results_df():
    # Use `.collect()` to get our result (in this case, getting the first 100 rows)
    return render.DataGrid(load_data_module.df.head(100).collect())
```

You might notice that the syntax has changed a little bit. We use `scan_parquet()` instead of `read_parquet` to return a Lazy Frame instead of a DataFrame. This means that when we import our data in the module, we're not actually loading it in yet.
Polars waits until it sees the `.collect()` to execute the sequence of commands to
1. Read in the data
2. Get the first 100 rows (`.head(100)`) and then
3. `.collect()` the results.

In this case, that's immediately since we're rendering the data frame as soon as we load the app.
However, there may be cases where you delay execution. For example, you might wait until after you've collected user-inputs or users have clicked a button.

Let's look at an example with the [The Weather Dataset](https://www.kaggle.com/datasets/guillemservera/global-daily-climate-data).
This dataset is much, much bigger (nearly 28M rows) and so we'll take full advantage of Polars Lazy API to keep our app fast.

::: callout-note
### Modules and Laziness
Remember how before we had to put our data in a module? Now that we're only loading our data and querying when we need it (when we've called `.collect`), we don't need to worry about that!

If you're using the Lazy API or reading from a database, you can create that connection to your data in a module, or not. It's up to you.
:::

#### Example Application: Polars

Here's a simple app that takes advantage of Polars Lazy API. In this example, we have the dataset on our computer, but we could also easily load from a shared, remote location using a tool like [Pins](https://rstudio.github.io/pins-python/) or a package like [boto3](https://docs.aws.amazon.com/code-library/latest/ug/python_3_s3_code_examples.html) to read from AWS S3.

```python
from shiny.express import ui, render, input
import polars as pl

# Scan the parquet file. It won't be truly read until it's needed.
df = pl.scan_parquet("./daily_weather.parquet")

ui.page_opts(title="Weather", full_width=True, id="page")

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Fall", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city",
    "City",
    choices=df.select("city_name").unique().collect().to_series().to_list(),
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    results = (
        df.filter(pl.col("city_name") == input.city())
        .filter(pl.col("season").is_in(input.season()))
        .collect()
    )
    return render.DataGrid(results)

```

::: callout-note
### You can read from a database in Polars!
Polars can read from a databases too! It's important to note though that Polars essentially reads in the database table like a file, you'll still need to have it on disk (saved locally). For best results, try and scope down your initial query to pull the minimum data needed for your application and if you require many database queries and user inputs, consider a more sophisticated connector.

<details>
<summary>Show example code</summary>

```{.python filename="load_data.py"}
import polars as pl

uri = "postgresql://user:password@hostname/database_name"
# This is where you should scope down your query if you can.
query = "SELECT * FROM weather"

# Notice that this uses `read_*`.
# Polars cannot read the data lazily from the database, so this will load the entire query into memory.
# That's why we've put this into a module again.
data = pl.read_database_uri(query, uri)
```

```python
from shiny.express import ui, render, input
import polars as pl
from load_data import data

# Notice that this uses `read_*`. Polars cannot read the data lazily from the database, so this will load the entire query into memory.
df = pl.LazyFrame(data)

ui.page_opts(title="Weather", full_width=True, id="page")

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Fall", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city",
    "City",
    choices=df.select("city_name").unique().collect().to_series().to_list(),
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    results = (
        df.filter(pl.col("city_name") == input.city())
        .filter(pl.col("season").is_in(input.season()))
        .collect()
    )
    return render.DataGrid(results)

```
</details>
:::

### Working with remote data and databases

We can make this same application using a remote datastore. In this example, we'll use Ibis and Postgres, but you could easily substitute another database or a data warehouse like Snowflake or Databricks. Similarly, Ibis is the connector we will demonstrate, but you can substitute whatever database connector works best for your use case.

Regardless of what remote database solution or connector we use here, the anatomy of the application is roughly the same. We will take the user-inputs from the UI and pass them to the database for execution rather than keeping the *whole* dataset in the application for processing.

::: {.callout-tip collapse="true"}
## Advanced: Ibis + Polars + Postgres
You might be wondering at this point, could I use Ibis to pull data, then Polars to manipulate it and get the benefits of both? Yes and no.

This might be a helpful approach in a situation where you can break your data easily into segments (ex. an analysis of all data prior to the year 2000 and all data after) and are planning to do a lot of analytics in your application itself.
However, Ibis and Polars have very similar syntax to their calcuations. For applications with less memory-consuming analytics, it is likely not worth the headache.
:::

#### Example Application: Ibis Connecting to Remote Database (Postgres)
```python
from shiny.express import ui, render, input
import ibis

# Connect to the database
con = ibis.postgres.connect(
    user="", password="", host="", port=, database=""
)
df = con.table("weather")

ui.page_opts(title="Weather", full_width=True, id="page")

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Autumn", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city", "City", choices=df[["city_name"]].distinct().execute()["city_name"].tolist()
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    results = df.filter(
        [df.city_name == input.city(), df.season.isin(input.season())]
    ).execute()
    return render.DataGrid(results)
```

This syntax should look pretty familiar! Polars and Ibis have similar syntax, but you'll notice we cue the execution of operations (filters, etc.) on our lazy tables using `.execute()` when we use Ibis.
:::

## Best practices

Now that you have data in your application, it's worthwhile to consider reading up on a few topics. You might consider this article on [mutability](link). This is likely to come up as you store and update filtered datasets.

You might also consider visiting our sections on [reactivity](link) as you think about how filtering your dataset should affect your other outputs.

We also have numerous full examples and starter templates of dashboards using different datastores. We've sorted them into categories roughly corresponding to the tech stack at the bottom of this article.

## What about writing to a data store?

The scenarios we've covered above are focused on reading in data to an application and performing queries with user-inputs. If you need to *write* data to the database, this introduces additional considerations we'll cover in a separate article on persistant storage in Shiny apps.


### Example Applications {#read-data-examples}
::: {.panel-tabset .panel-pills}

#### Polars
TODO: Do we have any of these?

#### DuckDB
Outliers app: https://github.com/skaltman/outliers-app-db-python/blob/main/app.py
Database Explorer: https://shiny.posit.co/py/templates/database-explorer/

#### Pandas
AWS-Community-Builders-App: https://github.com/robertgv/aws-community-builders-dashboard
Restaurant Tips: https://shiny.posit.co/py/templates/dashboard-tips/

#### Sqlite3
Database monitoring app: https://github.com/posit-dev/py-shiny-templates/tree/main/monitor-database
:::
