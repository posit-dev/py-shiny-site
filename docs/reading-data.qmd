---
title: Reading data in Shiny
editor:
  markdown:
    wrap: sentence
lightbox:
  effect: fade
---

From CSVs to parquet files to cloud-hosted databases, if you can access it from Python, you can access it from Shiny. We won't go through every possible configuration here, but we'll choose a handful to explore. Our examples will start simple and build in complexity.

## Reading data into memory {#read-into-memory}

A common pattern in Shiny apps is reading in data from a file on your computer, such as a CSV. We use an example CSV in the example below, but the strategy holds for other file formats you might want to use.

### Examples
::: {.panel-tabset .panel-pills}

### Polars
[Polars](https://pola.rs/) is an open-source library for data manipulation. It provides tooling to make data processing fast and intuitive. It is particularly helpful for large datasets, as its optimization engine can significantly speed up costly processes.

```python
df = pl.read_csv(Path(__file__).parent / "test_data.csv")
```

### Pandas
[Pandas](https://pandas.pydata.org/) is one of the most widely used open-source data analysis libraries.

```python
df = pd.read_csv(Path(__file__).parent / "test_data.csv")
```

### DuckDB
[DuckDB](https://duckdb.org/) is an open-source, in process database system which allows you to perform data analysis tasks quickly and with SQL syntax. Its memory usage optimizations make complex calculations quicker to run.

```python
df = duckdb.read_csv(Path(__file__).parent / "test_data.csv").df()
```
:::

::: callout-note
### Remember: Put your data in a module!

For better performance in situations where you are loading an entire dataset into memory, it is best to load your data from within a separate module and import it in your `app.py` file. This makes sure it is only loaded once. For more info why, see the Express documentation [here](express-in-depth.qmd#shared-objects) but for now, let's just make sure we read our csv in a separate module, then import it in our `app.py`.

<details>
<summary>Show example code</summary>

```{.python filename="app.py"}
from shiny.express import ui, render
import data_load_module

ui.page_opts(title="Data", full_width=True, id="page")

@render.data_frame
def results_df():
    return render.DataGrid(data_load_module.df)
```
```{.python filename="load_data.py"}
import polars as pl
from pathlib import Path

df = pl.read_csv(Path(__file__).parent / "test_data.csv")
```
</details>
:::

## Databases and larger-than-memory data {#read-db}
Some datasets are too large to fit cleanly in memory or don't live on our computer. Perhaps you want to connect to a remote database or perform complicated filtering operations on a large dataset.

### Examples
::: {.panel-tabset .panel-pills}
### Polars LazyFrame
```python
df = pl.scan_csv(Path(__file__).parent / "test_data.csv")

# Example query
query_result = df.head(100).collect()
```

### Ibis
[Ibis](https://ibis-project.org/) provides a shared dataframe interface for a wide variety of backends, including DuckDB, Postgres, BigQuery, etc.

We'll use Postgres in the below example to demonstrate connecting to a remote data source.

```bash
pip install 'ibis-framework[postgres]'
```

```python
import ibis

con = ibis.postgres.connect(
    user="username",
    password="password",
    host="hostname",
    port=5432,
    database="database",
)
my_table = con.table("mytablename")

# This is when Ibis will perform your query operations
query_result = my_table.head(100)

```

### SQLAlchemy
SQLAlchemy allows for more fine-grained SQL usage with either plain SQL text execution or SQLAlchemy's own Object-Relational Mapping syntax, which allows you to use a similar syntax to Polars and Ibis.

SQLAlchemy is the most flexible method of all of these, but with this flexibility comes much greater complexity.

#TODO: Add ORM example
```python
from sqlalchemy import create_engine, text

# Note: You will need to also install psycopg2 (or specify another supported connector)
engine = create_engine('postgresql://user:password@hostname/database_name')
with engine.connect() as conn:
    result = conn.execute(text("SELECT * FROM tablename LIMIT 100"))
```
:::

In both cases, we define our query (our data manipulations) and then rely on an external 'engine' to optimize it, run it, and provide us with the results.

We'll go over two examples here:

1. You have a large amount of data available on your computer that you want to use in a Shiny app. This is a great case for Polars *Lazy API*, which optimizes data manipulation tasks to run as quickly and using as little memory as possible.
2. You are connecting to a remote database. Your query will run remotely on that database and return a result to you (ex. Ibis with remote database, SQLAlchemy with remote database)

Let's start with the first case.

### Large datasets and Polars' Lazy API

#### Lazy Evaluation

Polars has a great toolset for efficiently working with large data called the *lazy API*. The lazy API takes your query as a whole instead of line-by-line and processes only what's needed when it's needed. It does this using its internal query optimizer and uses streaming to work with data sets larger than could otherwise fit in memory.

Let's modify our work from above to use the Lazy API.

```{.python filename="load_data.py"}
import polars as pl
from pathlib import Path

# Use `scan_*` instead of `read_*` to use the lazy API
df = pl.scan_parquet(Path(__file__).parent / "filename.parquet")
```

```{.python filename="app.py"}
from shiny.express import ui, render
import load_data_module

ui.page_opts(title="My Table", full_width=True, id="page")


@render.data_frame
def results_df():
    # Use `.collect()` to get our result (in this case, getting the first 100 rows)
    return render.DataGrid(load_data_module.df.head(100).collect())
```

You might notice that the syntax has changed a little bit. We use `scan_parquet()` instead of `read_parquet` to return a Lazy Frame instead of a DataFrame. This means that when we import our data in the module, we're not actually loading it in yet.
Polars waits until it sees the `.collect()` to execute the sequence of commands to
1. Read in the data
2. Get the first 100 rows (`.head(100)`) and then
3. `.collect()` the results.

In this case, that's immediately since we're rendering the data frame as soon as we load the app.
However, there may be cases where you delay execution. For example, you might wait until after you've collected user-inputs or users have clicked a button.

Let's look at an example with the [The Weather Dataset](https://www.kaggle.com/datasets/guillemservera/global-daily-climate-data).
This dataset is much, much bigger (nearly 28M rows) and so we'll take full advantage of Polars Lazy API to keep our app fast.

::: callout-note
### Modules and Laziness
Remember how before we had to put our data in a module? Now that we're only loading our data and querying when we need it (when we've called `.collect`), we don't need to worry about that!

If you're using the Lazy API or reading from a database, you can create that connection to your data in a module, or not. It's up to you.
:::

#### Example Application: Polars

Here's a simple app that takes advantage of Polars Lazy API:

TODO: Add run it in shiny live link?
Callout: You can read from a db here too!

```python
from shiny.express import ui, render, input
import polars as pl

# Scan the parquet file. It won't be truly read until it's needed.
df = pl.scan_parquet("./daily_weather.parquet")

ui.page_opts(title="Weather", full_width=True, id="page")

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Fall", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city",
    "City",
    choices=df.select("city_name").unique().collect().to_series().to_list(),
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    results = (
        df.filter(pl.col("city_name") == input.city())
        .filter(pl.col("season").is_in(input.season()))
        .collect()
    )
    return render.DataGrid(results)

```

::: callout-note
### You can read from a database in Polars!
Polars can read from a databases too! It's important to note though that Polars essentially reads in the database table like a file--you'll still need to have it in memory. For best results, try and scope down your initial query to pull the minimum data needed for your application and if you require many database queries and user inputs, consider a more sophisticated connector.

<details>
<summary>Show example code</summary>

```{.python filename="load_data.py"}
import polars as pl

uri = "yourURIhere"
# This is where you should scope down your query if you can.
query = "SELECT * FROM weather"

# Notice that this uses `read_*`. Polars cannot read the data lazily from the database, so this will load the entire query into memory. That's why we've put this into a module again.
data = pl.read_database_uri(query, uri)
```

```python
from shiny.express import ui, render, input
import polars as pl
from load_data import data

# Notice that this uses `read_*`. Polars cannot read the data lazily from the database, so this will load the entire query into memory.
df = pl.LazyFrame(data)

ui.page_opts(title="Weather", full_width=True, id="page")

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Fall", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city",
    "City",
    choices=df.select("city_name").unique().collect().to_series().to_list(),
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    results = (
        df.filter(pl.col("city_name") == input.city())
        .filter(pl.col("season").is_in(input.season()))
        .collect()
    )
    return render.DataGrid(results)

```
</details>
:::

### Working with remote data and databases

We can make this same application using Ibis and a remote data store:

* You can get data in two ways or through Ibis' syntax to do the initial query and then pull a chunk

Callout: There's no magical way to go from db to polars, if you want true lazy execution with db, need to be going through Ibis, to lazyframe it, this is nice if there's a big split in our data.
Otherwise you need to read into memory or preprocessing

::: {.panel-tabset .panel-pills}


### Postgres -> Ibis -> Polars





### Postgres -> Ibis
```python
from shiny.express import ui, render, input
import ibis

# Connect to the database
con = ibis.postgres.connect(
    user="", password="", host="", port=, database=""
)
df = con.table("weather")

ui.page_opts(title="Weather", full_width=True, id="page")

# Create a checkbox input so our users can select one or all seasons
ui.input_checkbox_group(
    "season",
    "Season",
    choices=["Summer", "Winter", "Autumn", "Spring"],
    selected="Summer",
)

# Create a multi-select input so users can select a city from those in our dataset
ui.input_selectize(
    "city", "City", choices=df[["city_name"]].distinct().execute()["city_name"].tolist()
)

# Filter the data frame based on the user inputs above
@render.data_frame
def results_df():
    results = df.filter(
        [df.city_name == input.city(), df.season.isin(input.season())]
    ).execute()
    return render.DataGrid(results)
```

This syntax should look pretty familiar! Polars and Ibis have similar syntax, but you'll notice we cue the execution of operations (filters, etc.) on our lazy tables using `.execute()` when we use Ibis.

:::

END IMPORTING DATA ARTICLE

Top level:
Importing Data
    Potentially have a 'my data lives remotely' skip down to bottom link
    x * In-memory data importing
    x * Out of memory/lazyframe/remote data
        * Callout a few different options
            * Ibis
            * SQLAlchemy
            * Polars Lazyframe
            * Something else?
        * Lazyframe
            * How we interact with this in the app based on when it becomes in memory
        * Now our data is totally remote:
            * Ibis example (local connect) supports in memory too callout
            * SQLAlchemy remote (postgres? in AWS RDS)
            * Pins callout supports in memory too callout
            * Recommendation for connecting to data warehouses

Doing stuff to data - callout to mutability article
Persistant Storage/Writing
* Callout connect cloud and support for secrets and persistent storage, link to docs, tehre's a talk that aleks chisolm did at conf



## What about writing to a data store? When to consider persistent storage



### Example Applications
::: {.panel-tabset .panel-pills}

#### Polars


#### DuckDB
Outliers app: Reads and writes from DuckDB https://github.com/skaltman/outliers-app-db-python/blob/main/app.py
Database Explorer: https://shiny.posit.co/py/templates/database-explorer/

#### Pandas
AWS-Community-Builders-App: Process data from a CSV and displays it https://github.com/robertgv/aws-community-builders-dashboard

#### Sqlite3
Database monitoring app: https://github.com/posit-dev/py-shiny-templates/tree/main/monitor-database
